{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f57529",
   "metadata": {},
   "source": [
    "# Data Ingestion - Bronze Layer\n",
    "\n",
    "## Overview\n",
    "This notebook implements the **Bronze Layer** of the Medallion Architecture for the Capstone Data Pipeline. It ingests raw data from multiple sources (transactions, customers, and products) and creates initial Delta tables with schema validation, data rescue capabilities, and audit logging.\n",
    "\n",
    "## Objective\n",
    "- Ingest raw CSV and JSON data from landing zones\n",
    "- Apply predefined schemas to validate incoming data\n",
    "- Implement schema evolution and data rescue mechanisms\n",
    "- Create Bronze-level Delta tables for downstream processing\n",
    "- Maintain audit trails for data lineage tracking\n",
    "\n",
    "## Architecture\n",
    "The notebook follows the **Medallion Architecture** pattern:\n",
    "- **Bronze**: Raw data ingestion layer (this notebook)\n",
    "- **Silver**: Cleaned and validated data\n",
    "- **Gold**: Business-ready aggregated data\n",
    "\n",
    "## Key Features\n",
    "- **Schema Evolution**: Uses Databricks Auto Loader with `rescue` mode to handle schema changes\n",
    "- **Data Rescue**: Captures unexpected columns in `_rescue` column for debugging\n",
    "- **Audit Logging**: Tracks all ingestion events with timestamps\n",
    "- **Structured Schemas**: Enforces strict typing on ingestion\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "### 1. Products\n",
    "- **Format**: CSV\n",
    "- **Path**: `/Volumes/capstone/bronze/raw/files/landing/products_raw`\n",
    "- **Fields**: item_id, product_name, category\n",
    "\n",
    "### 2. Transactions\n",
    "- **Format**: CSV\n",
    "- **Path**: `/Volumes/capstone/bronze/raw/files/landing/transactions_raw`\n",
    "- **Fields**: order_id, item_id, quantity, price, order_timestamp, corrupted_flag\n",
    "\n",
    "### 3. Customers\n",
    "- **Format**: JSON (multiline)\n",
    "- **Path**: `/Volumes/capstone/bronze/raw/files/landing/customers_raw`\n",
    "- **Fields**: customer_id, name, contact (nested: email), region\n",
    "\n",
    "## Output Tables\n",
    "| Table Name | Location | Format |\n",
    "|-----------|----------|--------|\n",
    "| `capstone.bronze.products` | Managed Delta | Delta |\n",
    "| `capstone.bronze.transaction` | Managed Delta | Delta |\n",
    "| `capstone.bronze.customer` | Managed Delta | Delta |\n",
    "\n",
    "## Audit Logging\n",
    "- **Log Location**: `/Volumes/capstone/bronze/history`\n",
    "- **Contents**: Table creation events with timestamps, row counts, and status\n",
    "- Accessible via Delta table format for querying\n",
    "\n",
    "## Configuration Parameters\n",
    "\n",
    "### Path Variables\n",
    "- `base_raw_path`: Root path for raw data volumes\n",
    "- `checkpointlocation`: Checkpoint directory for streaming operations\n",
    "- `log_path`: Audit log storage location\n",
    "\n",
    "### Processing Options\n",
    "- **CSV Options**: Format detection, header parsing, schema evolution with rescue mode\n",
    "- **JSON Options**: Multiline support, format detection, schema evolution with rescue mode\n",
    "\n",
    "## Dependencies\n",
    "- **Spark SQL**: Core data processing\n",
    "- **Databricks Auto Loader**: Incremental data ingestion\n",
    "- **Delta Lake**: Table format and ACID transactions\n",
    "- **capstone_pipeline.main**: Custom functions for table creation and logging\n",
    "\n",
    "## Usage Notes\n",
    "- Ensure all source paths exist before running\n",
    "- Review audit logs after ingestion for any data rescue events\n",
    "- Monitor `_rescue` columns for data quality issues\n",
    "- Checkpoint locations prevent duplicate processing in streaming scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd090a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%skip` not found.\n"
     ]
    }
   ],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"capstone\", \"Enter the Catalog: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4a66f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required PySpark data types for schema definition\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6adbd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 1, pos 12)\n\n== SQL ==\nuse catalog {dbutils.widgets.get(\"catalog\")}\n------------^^^\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.parser.ParseException\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:479)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:120)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:167)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$7(SparkSession.scala:824)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$6(SparkSession.scala:824)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:698)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3766)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3590)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3433)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParseException\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m sqlcmd= \u001b[33m'\u001b[39m\u001b[33muse catalog \u001b[39m\u001b[33m{\u001b[39m\u001b[33mdbutils.widgets.get(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcatalog\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m)}\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlcmd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\capstone_pipeline_etl\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\session.py:687\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args)\u001b[39m\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery: \u001b[38;5;28mstr\u001b[39m, args: Optional[Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], List]] = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[33m\"\u001b[39m\u001b[33mDataFrame\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    686\u001b[39m     cmd = SQL(sqlQuery, args)\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m     data, properties = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    688\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msql_command_result\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[32m    689\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(CachedRelation(properties[\u001b[33m\"\u001b[39m\u001b[33msql_command_result\u001b[39m\u001b[33m\"\u001b[39m]), \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\capstone_pipeline_etl\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1304\u001b[39m, in \u001b[36mSparkConnectClient.execute_command\u001b[39m\u001b[34m(self, command, observations, extra_request_metadata)\u001b[39m\n\u001b[32m   1302\u001b[39m     req.user_context.user_id = \u001b[38;5;28mself\u001b[39m._user_id\n\u001b[32m   1303\u001b[39m req.plan.command.CopyFrom(command)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m data, _, _, _, properties = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_request_metadata\u001b[49m\n\u001b[32m   1306\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1308\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (data.to_pandas(), properties)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\capstone_pipeline_etl\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1749\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch\u001b[39m\u001b[34m(self, req, observations, extra_request_metadata, self_destruct)\u001b[39m\n\u001b[32m   1746\u001b[39m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1748\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers=\u001b[38;5;28mself\u001b[39m._progress_handlers, operation_id=req.operation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1750\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_request_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\n\u001b[32m   1751\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1752\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1753\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\capstone_pipeline_etl\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1725\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req, observations, extra_request_metadata, progress)\u001b[39m\n\u001b[32m   1723\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[32m   1724\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1725\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\capstone_pipeline_etl\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:2041\u001b[39m, in \u001b[36mSparkConnectClient._handle_error\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m   2039\u001b[39m \u001b[38;5;28mself\u001b[39m.thread_local.inside_error_handling = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2040\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc.RpcError):\n\u001b[32m-> \u001b[39m\u001b[32m2041\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2042\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   2043\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCannot invoke RPC\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\capstone_pipeline_etl\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:2143\u001b[39m, in \u001b[36mSparkConnectClient._handle_rpc_error\u001b[39m\u001b[34m(self, rpc_error)\u001b[39m\n\u001b[32m   2128\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[32m   2129\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mPython versions in the Spark Connect client and server are different. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2130\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mTo execute user-defined functions, client and server should have the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2139\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2140\u001b[39m                 )\n\u001b[32m   2141\u001b[39m             \u001b[38;5;66;03m# END-EDGE\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2143\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[32m   2144\u001b[39m                 info,\n\u001b[32m   2145\u001b[39m                 status.message,\n\u001b[32m   2146\u001b[39m                 \u001b[38;5;28mself\u001b[39m._fetch_enriched_error(info),\n\u001b[32m   2147\u001b[39m                 \u001b[38;5;28mself\u001b[39m._display_server_stack_trace(),\n\u001b[32m   2148\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2150\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status.message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2151\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mParseException\u001b[39m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near '{'. SQLSTATE: 42601 (line 1, pos 12)\n\n== SQL ==\nuse catalog {dbutils.widgets.get(\"catalog\")}\n------------^^^\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.parser.ParseException\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:479)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:120)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:167)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$7(SparkSession.scala:824)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$6(SparkSession.scala:824)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:698)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3766)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3590)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3433)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
     ]
    }
   ],
   "source": [
    "sqlcmd= f'CREATE catalog IF NOT EXISTS {dbutils.widgets.get(\"catalog\")}'\n",
    "spark.sql(sqlcmd)\n",
    "\n",
    "sqlcmd= f'use catalog {dbutils.widgets.get(\"catalog\")}'\n",
    "spark.sql(sqlcmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a15f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlcmd= f'CREATE SCHEMA IF NOT EXISTS {dbutils.widgets.get(\"catalog\")}.bronze'\n",
    "spark.sql(sqlcmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7be1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base paths for raw data ingestion and checkpoint locations\n",
    "base_raw_path = f\"/Volumes/{dbutils.widgets.get(\"catalog\")}\"\n",
    "landing_path = f\"{base_raw_path}/bronze/raw/files/landing\"\n",
    "transactions_path = f\"{landing_path}/transactions_raw\"\n",
    "customers_path = f\"{landing_path}/customers_raw\"\n",
    "products_path = f\"{landing_path}/products_raw\"\n",
    "checkpointlocation = f\"{base_raw_path}/meta/checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa61e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for transactions data\n",
    "# Fields: order_id, item_id, quantity, price, order_timestamp, corrupted_flag\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"item_id\", StringType(), True),\n",
    "    StructField(\"quantity\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    StructField(\"order_timestamp\", StringType(), True),\n",
    "    StructField(\"corrupted_flag\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219dde73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for customers data\n",
    "# Includes nested contact structure with email field\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"contact\", StructType([\n",
    "        StructField(\"email\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7a1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for products data\n",
    "# Fields: item_id, product_name, category\n",
    "products_schema = StructType([\n",
    "    StructField(\"item_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5501eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Databricks Auto Loader options for CSV and JSON ingestion\n",
    "# cloudFiles.schemaEvolutionMode = \"rescue\" captures unexpected columns in _rescue column\n",
    "csv_options = {\n",
    "    \"cloudFiles.format\": \"csv\",\n",
    "    \"header\": \"true\",\n",
    "    \"cloudFiles.schemaEvolutionMode\": \"rescue\",\n",
    "    \"rescuedDataColumn\": \"_rescue\"\n",
    "}\n",
    "\n",
    "# JSON options support multiline JSON documents with schema rescue mode\n",
    "json_options = {\n",
    "    \"cloudFiles.format\": \"json\",\n",
    "    \"multiline\": \"true\",\n",
    "    \"cloudFiles.schemaEvolutionMode\": \"rescue\",\n",
    "    \"rescuedDataColumn\": \"_rescue\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e784cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper functions from the capstone_pipeline module\n",
    "# Functions: create_table_bronze (creates Bronze layer Delta tables), audit_log (logs table creation events)\n",
    "from capstone_pipeline.main import create_table_bronze, audit_log\n",
    "\n",
    "# Define path for audit logs\n",
    "log_path = f\"/Volumes/{dbutils.widgets.get(\"catalog\")}/meta/history\"\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {dbutils.widgets.get(\"catalog\")}.bronze.history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bronze layer products table from CSV data with schema validation and rescue mode\n",
    "table_name = f'{dbutils.widgets.get(\"catalog\")}.bronze.products'\n",
    "chkpnt_path = checkpointlocation + \"/bronzeproduct\"\n",
    "create_table_bronze(spark, products_schema, products_path, chkpnt_path, csv_options, table_name)\n",
    "audit_log(spark, table_name, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cea0bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bronze layer transactions table from CSV data with schema validation and rescue mode\n",
    "table_name = f'{dbutils.widgets.get(\"catalog\")}.bronze.transactions'\n",
    "chkpnt_path = checkpointlocation + \"/bronzetransaction\"\n",
    "create_table_bronze(spark, transactions_schema, transactions_path, chkpnt_path, csv_options, table_name)\n",
    "audit_log(spark, table_name, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451d7198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bronze layer customers table from multiline JSON data with schema validation and rescue mode\n",
    "table_name = f'{dbutils.widgets.get(\"catalog\")}.bronze.customers'\n",
    "chkpnt_path = checkpointlocation + \"/bronzecustomer\"\n",
    "create_table_bronze(spark, customers_schema, customers_path, chkpnt_path, json_options, table_name)\n",
    "audit_log(spark, table_name, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8f8e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display audit logs to verify all tables were successfully created and ingested\n",
    "display(spark.read.format(\"delta\").load(log_path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
