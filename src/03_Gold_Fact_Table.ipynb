{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bacfeb07",
   "metadata": {},
   "source": [
    "# Gold Fact Build - Daily Sales Fact\n",
    "\n",
    "## Summary\n",
    "- Purpose: Join Silver-level transaction, customer SCD, and product dimensions to build the gold fact table used for reporting.\n",
    "- Inputs: `capstone.silver.transactions`, `capstone.silver.customers_scd2`, `capstone.silver.products`\n",
    "- Outputs: `capstone.gold.daily_sales_fact` (enriched fact table)\n",
    "- Audit: Calls `audit_log(spark, table_name, log_path)` after table creation or insert operations.\n",
    "\n",
    "## Key Transformations\n",
    "- Enrich transactions with customer and product dimensions\n",
    "- Compute `line_total` and capture `order_date`\n",
    "\n",
    "## Usage\n",
    "- Run after Silver tables are ready; ensure correct clustering/partitioning for performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"capstone\", \"Enter the Catalog: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5c4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col\n",
    "from capstone_pipeline.main import audit_log\n",
    "\n",
    "table_name = f'{dbutils.widgets.get(\"catalog\")}.gold.daily_sales_fact'\n",
    "log_path = f'/Volumes/{dbutils.widgets.get(\"catalog\")}/meta/history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd723198",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver_trans = spark.table(f\"{dbutils.widgets.get(\"catalog\")}.silver.transactions\")\n",
    "\n",
    "display(df_silver_trans.limit(10))\n",
    "\n",
    "join_condition_cust = (\n",
    "    (col(\"transaction.order_timestamp\") >= col(\"customer.start_date\")) &\n",
    "    ((col(\"customer.end_date\").isNull()) | (col(\"transaction.order_timestamp\") < col(\"customer.end_date\")))\n",
    ")\n",
    "\n",
    "df_silver_trans_staged = df_silver_trans.withColumn(\n",
    "    \"order_date\", \n",
    "    col(\"order_timestamp\").cast(\"date\"))\n",
    "\n",
    "df_txn_with_customer_product = (\n",
    "    df_silver_trans_staged.alias(\"transaction\")\n",
    "    .join(spark.table(f\"{dbutils.widgets.get(\"catalog\")}.silver.customers_scd2\")\n",
    "        .alias(\"customer\"), join_condition_cust, how=\"left\")\n",
    "    .join(spark.table(f\"{dbutils.widgets.get(\"catalog\")}.silver.products\")\n",
    "        .alias(\"product\"), on=\"item_id\", how=\"left\")\n",
    "    .select(\n",
    "        col(\"transaction.order_id\"),\n",
    "        col(\"transaction.order_date\"),\n",
    "        col(\"transaction.item_id\"),\n",
    "        col(\"product.product_name\"),\n",
    "        col(\"product.category\").alias(\"product_category\"),\n",
    "        col(\"customer.customer_key\").alias(\"customer_key\"),\n",
    "        col(\"customer.customer_id\").alias(\"customer_id\"),\n",
    "        col(\"customer.name\").alias(\"customer_name\"),\n",
    "        col(\"customer.email\").alias(\"customer_email\"),\n",
    "        col(\"customer.region\").alias(\"customer_region\"),\n",
    "        col(\"transaction.quantity\"),\n",
    "        col(\"transaction.price\"),\n",
    "        (col(\"transaction.quantity\") * col(\"transaction.price\")).alias(\"line_total\"),\n",
    "        col(\"transaction.order_timestamp\")))\n",
    "\n",
    "df_txn_with_customer_product.createOrReplaceTempView(\"vw_txn_with_customer_product\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd0e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(f\"{dbutils.widgets.get(\"catalog\")}.gold.daily_sales_fact\"):\n",
    "\n",
    "    spark.sql(f\"\"\"CREATE OR REPLACE TABLE {dbutils.widgets.get(\"catalog\")}.gold.daily_sales_fact\n",
    "    USING DELTA\n",
    "    CLUSTER BY (order_date, product_category, customer_id)\n",
    "    AS\n",
    "    SELECT\n",
    "    order_id,\n",
    "    order_date,\n",
    "    item_id,\n",
    "    product_name,\n",
    "    product_category,\n",
    "    customer_key,\n",
    "    customer_id,\n",
    "    customer_name,\n",
    "    customer_email,\n",
    "    customer_region AS region,\n",
    "    quantity,\n",
    "    price,\n",
    "    line_total,\n",
    "    order_timestamp\n",
    "    FROM vw_txn_with_customer_product\"\"\")\n",
    "\n",
    "else:\n",
    "    spark.sql(f\"\"\"INSERT INTO {dbutils.widgets.get(\"catalog\")}.gold.daily_sales_fact\n",
    "    SELECT\n",
    "    order_id,\n",
    "    order_date,\n",
    "    item_id,\n",
    "    product_name,\n",
    "    product_category,\n",
    "    customer_key,\n",
    "    customer_id,\n",
    "    customer_name,\n",
    "    customer_email,\n",
    "    customer_region AS region,\n",
    "    quantity,\n",
    "    price,\n",
    "    line_total,\n",
    "    order_timestamp\n",
    "    FROM vw_txn_with_customer_product\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09783ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_log(spark, table_name, log_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
