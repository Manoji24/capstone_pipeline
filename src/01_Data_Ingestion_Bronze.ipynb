{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f57529",
   "metadata": {},
   "source": [
    "# Data Ingestion - Bronze Layer\n",
    "\n",
    "## Overview\n",
    "This notebook implements the **Bronze Layer** of the Medallion Architecture for the Capstone Data Pipeline. It ingests raw data from multiple sources (transactions, customers, and products) and creates initial Delta tables with schema validation, data rescue capabilities, and audit logging.\n",
    "\n",
    "## Objective\n",
    "- Ingest raw CSV and JSON data from landing zones\n",
    "- Apply predefined schemas to validate incoming data\n",
    "- Implement schema evolution and data rescue mechanisms\n",
    "- Create Bronze-level Delta tables for downstream processing\n",
    "- Maintain audit trails for data lineage tracking\n",
    "\n",
    "## Architecture\n",
    "The notebook follows the **Medallion Architecture** pattern:\n",
    "- **Bronze**: Raw data ingestion layer (this notebook)\n",
    "- **Silver**: Cleaned and validated data\n",
    "- **Gold**: Business-ready aggregated data\n",
    "\n",
    "## Key Features\n",
    "- **Schema Evolution**: Uses Databricks Auto Loader with `rescue` mode to handle schema changes\n",
    "- **Data Rescue**: Captures unexpected columns in `_rescue` column for debugging\n",
    "- **Audit Logging**: Tracks all ingestion events with timestamps\n",
    "- **Structured Schemas**: Enforces strict typing on ingestion\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "### 1. Products\n",
    "- **Format**: CSV\n",
    "- **Path**: `/Volumes/capstone/bronze/raw/files/landing/products_raw`\n",
    "- **Fields**: item_id, product_name, category\n",
    "\n",
    "### 2. Transactions\n",
    "- **Format**: CSV\n",
    "- **Path**: `/Volumes/capstone/bronze/raw/files/landing/transactions_raw`\n",
    "- **Fields**: order_id, item_id, quantity, price, order_timestamp, corrupted_flag\n",
    "\n",
    "### 3. Customers\n",
    "- **Format**: JSON (multiline)\n",
    "- **Path**: `/Volumes/capstone/bronze/raw/files/landing/customers_raw`\n",
    "- **Fields**: customer_id, name, contact (nested: email), region\n",
    "\n",
    "## Output Tables\n",
    "| Table Name | Location | Format |\n",
    "|-----------|----------|--------|\n",
    "| `capstone.bronze.products` | Managed Delta | Delta |\n",
    "| `capstone.bronze.transaction` | Managed Delta | Delta |\n",
    "| `capstone.bronze.customer` | Managed Delta | Delta |\n",
    "\n",
    "## Audit Logging\n",
    "- **Log Location**: `/Volumes/capstone/bronze/history`\n",
    "- **Contents**: Table creation events with timestamps, row counts, and status\n",
    "- Accessible via Delta table format for querying\n",
    "\n",
    "## Configuration Parameters\n",
    "\n",
    "### Path Variables\n",
    "- `base_raw_path`: Root path for raw data volumes\n",
    "- `checkpointlocation`: Checkpoint directory for streaming operations\n",
    "- `log_path`: Audit log storage location\n",
    "\n",
    "### Processing Options\n",
    "- **CSV Options**: Format detection, header parsing, schema evolution with rescue mode\n",
    "- **JSON Options**: Multiline support, format detection, schema evolution with rescue mode\n",
    "\n",
    "## Dependencies\n",
    "- **Spark SQL**: Core data processing\n",
    "- **Databricks Auto Loader**: Incremental data ingestion\n",
    "- **Delta Lake**: Table format and ACID transactions\n",
    "- **capstone_pipeline.main**: Custom functions for table creation and logging\n",
    "\n",
    "## Usage Notes\n",
    "- Ensure all source paths exist before running\n",
    "- Review audit logs after ingestion for any data rescue events\n",
    "- Monitor `_rescue` columns for data quality issues\n",
    "- Checkpoint locations prevent duplicate processing in streaming scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd090a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%skip` not found.\n"
     ]
    }
   ],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"capstone\", \"Enter the Catalog: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4a66f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required PySpark data types for schema definition\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6adbd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m sqlcmd= \u001b[33m\"\u001b[39m\u001b[33muse catalog \u001b[39m\u001b[38;5;132;01m{catalog}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mspark\u001b[49m.sql(sqlcmd)\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "sqlcmd= \"use catalog {catalog}\"\n",
    "spark.sql(sqlcmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7be1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base paths for raw data ingestion and checkpoint locations\n",
    "base_raw_path = \"/Volumes/capstone/bronze/raw/files\"\n",
    "transactions_path = f\"{base_raw_path}/landing/transactions_raw\"\n",
    "customers_path = f\"{base_raw_path}/landing/customers_raw\"\n",
    "products_path = f\"{base_raw_path}/landing/products_raw\"\n",
    "checkpointlocation = f\"{base_raw_path}/checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa61e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for transactions data\n",
    "# Fields: order_id, item_id, quantity, price, order_timestamp, corrupted_flag\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"item_id\", StringType(), True),\n",
    "    StructField(\"quantity\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    StructField(\"order_timestamp\", StringType(), True),\n",
    "    StructField(\"corrupted_flag\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219dde73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for customers data\n",
    "# Includes nested contact structure with email field\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"contact\", StructType([\n",
    "        StructField(\"email\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7a1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for products data\n",
    "# Fields: item_id, product_name, category\n",
    "products_schema = StructType([\n",
    "    StructField(\"item_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5501eb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Databricks Auto Loader options for CSV and JSON ingestion\n",
    "# cloudFiles.schemaEvolutionMode = \"rescue\" captures unexpected columns in _rescue column\n",
    "csv_options = {\n",
    "    \"cloudFiles.format\": \"csv\",\n",
    "    \"header\": \"true\",\n",
    "    \"cloudFiles.schemaEvolutionMode\": \"rescue\",\n",
    "    \"rescuedDataColumn\": \"_rescue\"\n",
    "}\n",
    "\n",
    "# JSON options support multiline JSON documents with schema rescue mode\n",
    "json_options = {\n",
    "    \"cloudFiles.format\": \"json\",\n",
    "    \"multiline\": \"true\",\n",
    "    \"cloudFiles.schemaEvolutionMode\": \"rescue\",\n",
    "    \"rescuedDataColumn\": \"_rescue\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e784cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper functions from the capstone_pipeline module\n",
    "# Functions: create_table_bronze (creates Bronze layer Delta tables), audit_log (logs table creation events)\n",
    "from capstone_pipeline.main import create_table_bronze, audit_log, get_spark\n",
    "\n",
    "# Define path for audit logs\n",
    "log_path = \"/Volumes/capstone/bronze/history\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bronze layer products table from CSV data with schema validation and rescue mode\n",
    "table_name = \"bronze.products\"\n",
    "chkpnt_path = checkpointlocation + \"/bronzeproduct\"\n",
    "create_table_bronze(spark, products_schema, products_path, chkpnt_path, csv_options, table_name)\n",
    "audit_log(spark, table_name, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cea0bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bronze layer transactions table from CSV data with schema validation and rescue mode\n",
    "table_name = \"bronze.transaction\"\n",
    "chkpnt_path = checkpointlocation + \"/bronzetransaction\"\n",
    "create_table_bronze(spark, transactions_schema, transactions_path, chkpnt_path, csv_options, table_name)\n",
    "audit_log(spark, table_name, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451d7198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bronze layer customers table from multiline JSON data with schema validation and rescue mode\n",
    "table_name = \"bronze.customer\"\n",
    "chkpnt_path = checkpointlocation + \"/bronzecustomer\"\n",
    "create_table_bronze(spark, customers_schema, customers_path, chkpnt_path, json_options, table_name)\n",
    "audit_log(spark, table_name, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8f8e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display audit logs to verify all tables were successfully created and ingested\n",
    "display(spark.read.format(\"delta\").load(log_path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
