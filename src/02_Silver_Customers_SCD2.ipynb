{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2e0a2a",
   "metadata": {},
   "source": [
    "# Customers SCD2 Transformation - Silver Layer\n",
    "\n",
    "## Summary\n",
    "- Purpose: Build SCD Type 2 (slowly changing dimension) table for customers with historical tracking.\n",
    "- Inputs: `capstone.bronze.customers` Delta table\n",
    "- Outputs: `capstone.silver.customers_scd2` Delta table (SCD Type 2 history)\n",
    "- Audit: Calls `audit_log(spark, table_name, log_path)` after major merge/write operations.\n",
    "\n",
    "## Key Transformations\n",
    "- Compute hash values for dedup/equals\n",
    "- Merge staged data into SCD2 table using `MERGE` logic\n",
    "- Preserve historical records and mark `is_current` flags\n",
    "\n",
    "## Usage\n",
    "- Run after Bronze ingestion; ensure `customers_scd2` exists before merge operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"capstone\", \"Enter the Catalog: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235b6dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_timestamp, monotonically_increasing_id, when\n",
    "from capstone_pipeline.main import transform_hashvalue, audit_log\n",
    "\n",
    "\n",
    "table_name = f'{dbutils.widgets.get(\"catalog\")}.silver.customers_scd2'\n",
    "log_path = f'/Volumes/{dbutils.widgets.get(\"catalog\")}/meta/history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcfc3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "          CREATE TABLE IF NOT EXISTS {dbutils.widgets.get(\"catalog\")}.silver.customers_scd2 (\n",
    "            customer_key BIGINT,\n",
    "            customer_id STRING,\n",
    "            name STRING,\n",
    "            email STRING,\n",
    "            region STRING,\n",
    "            hash_value STRING,\n",
    "            start_date TIMESTAMP,\n",
    "            end_date TIMESTAMP,\n",
    "            is_current BOOLEAN,\n",
    "            _ingest_timestamp TIMESTAMP,\n",
    "            _source_file_name STRING\n",
    "            ) USING DELTA;\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fba284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfcustomers = spark.table('{dbutils.widgets.get(\"catalog\")}.bronze.customers')\n",
    "dfcustomers_staged = (dfcustomers\n",
    "            .drop(\"_rescue\")\n",
    "            .transform(transform_hashvalue, [\"name\", \"contact.email\", \"region\"])\n",
    "            .withColumn(\"email\", col(\"contact.email\"))\n",
    "            .withColumn(\"start_date\", current_timestamp())\n",
    "            .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "            .withColumn(\"is_current\", lit(True)))\n",
    "\n",
    "dfcustomers_staged.createOrReplaceTempView(\"dfcustomers_staged\")\n",
    "\n",
    "display(spark.sql(f\"SELECT * FROM dfcustomers_staged LIMIT 10\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b8c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcustscd = spark.table(f'{dbutils.widgets.get(\"catalog\")}.silver.customers_scd2')\n",
    "dfcustscdmaxkey = dfcustscd.agg({\"customer_key\": \"max\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b119a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfinsert = (dfcustomers_staged.alias(\"staged\")\n",
    "    .join(dfcustscd.filter(col(\"is_current\") == True).alias(\"src\"), on=[\"hash_value\",\"customer_id\"], how=\"inner\")\n",
    "    .select(\n",
    "        \"src.customer_key\",\n",
    "        \"staged.customer_id\",\n",
    "        \"staged.name\",\n",
    "        \"staged.email\",\n",
    "        \"staged.region\",\n",
    "        \"staged.hash_value\",\n",
    "        \"staged.start_date\",\n",
    "        \"staged._ingest_timestamp\",\n",
    "        \"staged._source_file_name\"\n",
    "    )\n",
    "    # .withColumn(\"customer_key\", lit(dfcustscdmaxkey.collect()[0][0]) + monotonically_increasing_id())\n",
    "    .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "    .withColumn(\"is_current\", lit(True)))\n",
    "\n",
    "dfinsert.createOrReplaceTempView(\"dfinsert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddafce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfinsert1 = (dfcustomers_staged.alias(\"staged\")\n",
    "    .join(dfcustscd.filter(col(\"is_current\") == True).alias(\"src\"), on=[\"hash_value\",\"customer_id\"], how=\"left\")\n",
    "    .select(\n",
    "        \"staged.customer_id\",\n",
    "        \"staged.name\",\n",
    "        \"staged.email\",\n",
    "        \"staged.region\",\n",
    "        \"staged.hash_value\",\n",
    "        \"staged.start_date\",\n",
    "        \"staged._ingest_timestamp\",\n",
    "        \"staged._source_file_name\"\n",
    "    )\n",
    "    .withColumn(\"customer_key\", lit(dfcustscdmaxkey.collect()[0][0]) + monotonically_increasing_id())\n",
    "    .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "    .withColumn(\"is_current\", lit(True)))\n",
    "\n",
    "dfinsert1.createOrReplaceTempView(\"dfinsert1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52788ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%skip \n",
    "merge_sql = f\"\"\"\n",
    "    MERGE INTO {dbutils.widgets.get(\"catalog\")}.silver.customers_scd2 AS target\n",
    "    USING dfinsert1 AS source\n",
    "    ON target.customer_id = source.customer_id AND target.is_current = TRUE AND target.customer_key = source.customer_key\n",
    "    WHEN NOT MATCHED THEN\n",
    "      INSERT (customer_key, customer_id, name, email, region, hash_value, start_date, end_date, is_current, _ingest_timestamp, _source_file_name)\n",
    "      VALUES (source.customer_key, source.customer_id, source.name, source.email, source.region, source.hash_value, source.start_date, source.end_date, source.is_current, source._ingest_timestamp, source._source_file_name)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(merge_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2324e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%skip \n",
    "audit_log(spark, table_name, log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_sql = f\"\"\"\n",
    "    MERGE INTO {dbutils.widgets.get(\"catalog\")}.silver.customers_scd2 AS target\n",
    "    USING (select customer_key, customer_id, name, email, region, hash_value, start_date, end_date, is_current, _ingest_timestamp, _source_file_name \n",
    "            from dfinsert1 \n",
    "           union \n",
    "           select customer_key, customer_id, name, email, region, hash_value, start_date, end_date, is_current, _ingest_timestamp, _source_file_name \n",
    "            from dfinsert) AS source\n",
    "    ON target.customer_id = source.customer_id AND target.is_current = TRUE AND target.customer_key = source.customer_key\n",
    "    WHEN MATCHED AND target.hash_value <> source.hash_value THEN\n",
    "      UPDATE SET target.is_current = FALSE, target.end_date = source.start_date\n",
    "    WHEN NOT MATCHED THEN\n",
    "      INSERT (customer_key, customer_id, name, email, region, hash_value, start_date, end_date, is_current, _ingest_timestamp, _source_file_name)\n",
    "      VALUES (source.customer_key, source.customer_id, source.name, source.email, source.region, source.hash_value, source.start_date, source.end_date, source.is_current, source._ingest_timestamp, source._source_file_name)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(merge_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b33cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_log(spark, table_name, log_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
